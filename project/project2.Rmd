---
title: 'Project 2: Modeling, Testing, and Predicting Bicycle Signal Safety'
author: "Carolina Baumanis"
date: '5/2021'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

# Data Overview

The dataset explored in this project comes from a bicycle signal installation experiment done in Austin, TX. At each of the 12 intersections, UT-CTR captured video recordings before and after bicycle silhouette signal faces were installed by the City of Austin. Each video captures at least 40 least 40 times when a cyclist and vehicle cross each other's path while traveling through the intersection (called an *interaction*). 

The main variables of interest in this study are safety related: running the red light and failure to yield. For this study, UT-CTR tallied cyclist/vehicle red light running, cyclist/vehicle yielded and did not yield, and various types of interactions between cyclists and vehicles. Red light running behaviors were further broken down into three types: gap acceptance (coming to a stop at a red light, and searching for a gap), signal jump (entering the intersection at the tail end of a red signal just before the green indication comes on), or other (generally, this means blasting through a solid red light). The interactions considered are one party reactions, two party reactions, conflict negotiated, and near misses. One and two parties reactions are when a cyclist and vehicle cross paths and either on or both parties changes their speed or position for reasons other than properly yielding the right of way. A conflict negotiated occurs when a cyclist and vehicle cross paths, and successfully yield the right of way to one another. Other data elements UT-CTR captured for each intersection include the hourly cyclist volumes, the hourly vehicle volumes, the type of bicycling facility, and the number of lanes perpendicular to the cyclist path.

## Required Libraries
First things first. Loading up the required libraries.
```{r}
library(tidyverse)
library(dplyr)
library(rstatix)
library(lmtest)
library(sandwich)
library(plotROC)
library(glmnet)
```

## Loading data
Next, reading in the data from the csv file and grabbing relevant columns
```{r}
bikes <- read_csv("bikes.csv")
bikes <- bikes
```

# MANOVA
For the experiment, we want to determine if there's a difference in any of the yielding, red light compliance, or interaction response variables by test bed, before versus after bicycle signal installation, or by treatment. A treatment is a combination of a test bed (6 test beds) and a scenario (before, after, or after 2). There's a total of 26 observations, and 133 variables measured for each observation. The 26 observations are a before, after, or second after scenario  at each of the 11 intersections.

The MANOVA considers the following variables: `PR1_hr`= one party reaction per hour, `PR2_hr` = two party reaction per hour, `NM_hr` = near miss per hour, `ConNeg_hr` = conflict negotiated (i.e. number of proper yields) per hour, `YieldVeh` = percentage of vehicles that yielding properly, `YieldBike` = percentage of bicycles that yielding properly, `BRRL_hr` = bicycle red light runs per hour, `VehSig_hr` = vehicle red light runs per hour. All variables that end in `_PC` are per potential conflict instead of per hour. A potential conflict is any time that a cyclist path and a vehicle past crossed within the intersection.

```{r}
bikes_m <- bikes %>% drop_na(End) %>% mutate(TestBed = as.factor(TestBed), YieldVeh = as.numeric(YieldVeh), YieldBike = as.numeric(YieldBike)) %>% mutate_at(which(grepl("_hr|_PC", names(bikes))), as.numeric)

#yielding, red light compliance, or interactions by test bed, 1 test
man <- manova(cbind(PR1_hr, PR1_PC, PR2_hr, PR2_PC, NM_hr, NM_PC, ConNeg_hr, ConNeg_PC, YieldVeh, YieldBike, BRRL_hr, BRRL_PC, VehSig_hr, VehSig_PC)~TestBed, data = bikes_m)
summary(man) 

#yielding, red light compliance, or interactions by scenario, 1 test
man1 <- manova(cbind(PR1_hr, PR2_hr, NM_hr, ConNeg_hr, YieldVeh, YieldBike, BRRL_hr, BRRL_PC, VehSig_hr, VehSig_PC)~Scenario, data = bikes_m)
summary(man1)

#yielding, red light compliance, or interactions by both scenario and test bed, 1 test
man2 <- manova(cbind(PR1_hr, PR2_hr, NM_hr, ConNeg_hr, YieldVeh, YieldBike, BRRL_hr, BRRL_PC, VehSig_hr, VehSig_PC)~SceTest, data = bikes_m)
summary(man2)

#trying to look for multivariate normality
ggplot(bikes_m, aes(x = YieldVeh, y = BRRL_PC)) +
  geom_point() + geom_density_2d() + facet_wrap(~TestBed, scales = "fixed")

```
The MANOVA considering each treatment `SceTest` as an explanatory variable shows that there is a least one significant difference for at least one of the response variables. The other two MANOVAs did not detect any significant differences. In general, MANOVA makes a ton of assumptions that are really hard to access and meet in practice. Some of the assumptions include, multivariate normality, homogeneity of variance, and no multicollinearity. From the plot, it doesn't appear the multivariate normality is met.

The univariate ANOVAs show that there's a mean difference in response for bicycle red light runs per hour `BRRL_hr` and per potential conflict `BRRL_PC`. Some of the pairwise comparisons are not possible because there's only one intersection (n=1) belonging to a specific treatment. The total number of relevant tests done on are 1 + 10 + 6 + 6 = 23 tests. Therefore, the probability of a type 1 error is 1 - 0.95^23 = 0.69, and the Bonferroni adjusted p-value is 0.05/23 = 0.002. Out of the possible comparisons, there are no significant differences in `BRRL_hr` or `BRRL_PC` after a Bonferroni adjustment.
```{r}
summary.aov(man2) #sig diff in bicycle red light runs per hr and per PC, 10 tests

#looking at mean differences in bicycle red light runs per hr
bikes_m %>% group_by(SceTest, TestBed) %>% summarize(mean(BRRL_hr))

#removing n = 1
bikes_m %>% group_by(SceTest) %>% count %>% filter(n>1)
bikes_rep <- bikes_m %>% group_by(SceTest) %>% filter(SceTest %in% c(1,2,12,13))

#bicycle red light runs per hour by treatment, 6 tests
pairwise.t.test(bikes_rep$BRRL_hr, bikes_rep$SceTest, p.adjust.method = "none")

#bicycle red light runs per potential conflict by treatment, 6 tests
pairwise.t.test(bikes_rep$BRRL_PC, bikes_rep$SceTest, p.adjust.method = "none")

```


# Randomization Test
A randomization test can help with seeing if there is a difference for before versus after in response variables we're interested. The nice thing about randomizations tests is we can do these comparisons in without having to worry about all the traditional assumptions that go with other parametric tests. With a randomization tests, there are no assumptions that could be violated because we're generating our own null distribution to compare our observations to and determine significance.

## Vehicle Failure to Yield
Does vehicle failure to yield differ in the before versus after, excluding after 2 scenarios? According to the randomization test, no there's no difference in before versus after.

```{r}

#how can I just make is before versus others? How can I do before versus after by testbed?

bikes_rand <- bikes %>% select(Scenario, YieldVeh) %>% filter(!Scenario == "A2")

#visualizing the distribution
ggplot(bikes_rand, aes(YieldVeh, fill = Scenario)) + 
  geom_histogram(bins = 8) +
  facet_wrap(~Scenario)

#ORIGINAL mean difference - slightly better vehicle yielding in the A = After scenario
bikes_rand %>% group_by(Scenario) %>% summarize(means = mean(YieldVeh)) %>% summarize('mean_diff' = diff(means))

#scrambling the columns to do the randomization test
rand_dist <- vector()

for(i in 1:5000){
  new <- data.frame(YieldVeh = sample(bikes_rand$YieldVeh), Scenario = bikes_rand$Scenario)
  rand_dist[i] <- mean(new[new$Scenario == "A",]$YieldVeh) - mean(new[new$Scenario == "B",]$YieldVeh)
}

#my OWN distribution under the null hypothesis
{hist(rand_dist, main = "", ylab = ""); abline(v = c(-0.0377522, 0.0377522), col = "red")}

#Testing the null hypothesis; p-value: fail to reject!
mean(rand_dist > 0.0377522 | rand_dist < -0.0377522)

```
## Hourly Cyclist Red Light Runs
Does hourly cyclist red light running behavior differ in the before versus after? According to the randomization test, no it does not differ significantly in the before and after scenarios.
```{r}
bikes_rand <- bikes %>% select(Scenario, BRRL_hr) %>% mutate(BRRL_hr = as.numeric(BRRL_hr)) %>% filter(!Scenario == "A2")

#visualizing the distribution
ggplot(bikes_rand, aes(BRRL_hr, fill = Scenario)) + 
  geom_histogram(bins = 8) +
  facet_wrap(~Scenario)

#ORIGINAL mean difference - slightly better vehicle yielding in the A = After scenario
bikes_rand %>% group_by(Scenario) %>% summarize(means = mean(BRRL_hr)) %>% summarize('mean_diff' = diff(means))

#scrambling the columns to do the randomization test
rand_dist <- vector()

for(i in 1:5000){
  new <- data.frame(BRRL_hr = sample(bikes_rand$BRRL_hr), Scenario = bikes_rand$Scenario)
  rand_dist[i] <- mean(new[new$Scenario == "A",]$BRRL_hr) - mean(new[new$Scenario == "B",]$BRRL_hr)
}

#my OWN distribution under the null hypothesis
{hist(rand_dist, main = "", ylab = ""); abline(v = c(-0.5866793, 0.5866793), col = "red")}

#Testing the null hypothesis; p-value: fail to reject!
mean(rand_dist > 0.5866793 | rand_dist < -0.5866793)
```

## Hourly Vehicle Red Light Runs
Does vehicle red light non-compliance per hour change for before versus after? According to the randomization test, it does not.
```{r}
#how can I just make is before versus others? How can I do before versus after by testbed?

bikes_rand <- bikes %>% select(Scenario, VehSig_hr) %>%  mutate(VehSig_hr = as.numeric(VehSig_hr)) %>% filter(!Scenario == "A2")

#visualizing the distribution
ggplot(bikes_rand, aes(VehSig_hr, fill = Scenario)) + 
  geom_histogram(bins = 8) +
  facet_wrap(~Scenario)

#ORIGINAL mean difference - slightly worse vehicle red light running in the A = After scenario
bikes_rand %>% group_by(Scenario) %>% summarize(means = mean(VehSig_hr)) %>% summarize('mean_diff' = diff(means))

#scrambling the columns to do the randomization test
rand_dist <- vector()

for(i in 1:5000){
  new <- data.frame(VehSig_hr = sample(bikes_rand$VehSig_hr), Scenario = bikes_rand$Scenario)
  rand_dist[i] <- mean(new[new$Scenario == "A",]$VehSig_hr) - mean(new[new$Scenario == "B",]$VehSig_hr)
}

#my OWN distribution under the null hypothesis
{hist(rand_dist, main = "", ylab = ""); abline(v = c(-0.4345455	, 0.4345455), col = "red")}

#Testing the null hypothesis; p-value: fail to reject!
mean(rand_dist > 0.4345455	 | rand_dist < -0.4345455)
```

# Linear Regression on Cyclist Red Light Runs

This linear regression is predicting cyclist red light compliance per hour `BRRL_hr` from bicycling hourly volumes per hour and scenarios. 

* Mean/predicted red light runs per hour at locations with zero hourly bicycle volume in the before scenario is 7.92162. Note that this interpretation doesn't make sense in reality because zero cyclists should mean zero cyclist red light runs. 

* For every one unit increase in hourly bicycle volume, cyclist red light runs per hour decrease for the before scenario by 0.10726.

* Scenario A with zero hourly bicycle volume have 0.65171 cyclist red light runs per hour predicted.

* Scenario A2 with zero hourly bicycle volume have 1.98899 cyclist red light runs per hour predicted.

* The slope of cyclist red light runs per hour on hourly bicycle volume is 0.03382 greater for Scenario A than for Scenario B.

* The slope of cyclist red light runs per hour on hourly bicycle volume is 0.09166 
greater for Scenario A2 than for Scenario B.

The model explains about 45% of the variability according to the adjusted r-squared.

```{r}
#change reference level to before B
bikes_m <- bikes_m %>% mutate(Scenario = factor(Scenario, levels = c("B", "A", "A2"))) 

bikes_m$BikePerHr_Bikes_c <- bikes_m$BikePerHr_Bikes - mean(bikes_m$BikePerHr_Bikes, na.rm = T)

#regression for cyclist red light runs per hour 
fit <- lm(BRRL_hr ~ BikePerHr_Bikes_c*Scenario, data = bikes_m) 
summary(fit)

bikes_m %>% select(BRRL_hr, TestBed, Scenario, BikePerHr_Bikes_c) %>% na.omit %>% ggplot(aes(BikePerHr_Bikes_c, BRRL_hr, color = Scenario)) + geom_point() + geom_smooth(method = "lm") 

``` 

In general, the qq plot shows the normality assumption is violated, and the scatterplot and the bptest show that homoskedasticity is NOT met. The values fan out as x increases, and the Breush-Pagan Test rejects the null hypothesis of homoskedasticity. 
```{r}

#Check linearity 
resids <- fit$residuals
fitvals <- fit$fitted.values
plot(fitvals, resids); abline(h = 0, col = "red")

#Check normality
par(mfrow = c(1,2)); hist(resids); qqnorm(resids); qqline(resids, col = 'red')

#Check homoskedasticity
bikes_m %>% select(BRRL_hr, TestBed, Scenario, BikePerHr_Bikes_c) %>% na.omit %>% ggplot(aes(BikePerHr_Bikes_c, BRRL_hr, color = Scenario)) + geom_point()

```

Since homoskedasticity is violated, robust SEs will help because they are robust against this violation. With the robust SEs, the intercept and hourly bicycle volume `BikePerHr_Bikes_c` are more significant than they were before. 
```{r}
#change reference level to before B
bikes_m <- bikes_m %>% mutate(Scenario = factor(Scenario, levels = c("B", "A", "A2"))) 

bikes_m$BikePerHr_Bikes_c <- bikes_m$BikePerHr_Bikes - mean(bikes_m$BikePerHr_Bikes, na.rm = T)

#regression for cyclist red light runs per hour 
fit <- lm(BRRL_hr ~ BikePerHr_Bikes_c*Scenario, data = bikes_m) 
coeftest(fit, vcovHC(fit))
```

# Linear Regression Red Light Runs & Bootsrapped SEs
Now, re-running the same regression model but with bootstrapped standard errors. The bootstrapped SEs are larger than what they were for for the original SEs and for the robust SEs. But, the null hypothesis is still rejected for this regression model since 0 is not contained the first two confidence interval parameter estimates (intercept and hourly bike volume). 
```{r}
#change reference level to before B
bikes_m <- bikes_m %>% mutate(Scenario = factor(Scenario, levels = c("B", "A", "A2"))) 

bikes_m$BikePerHr_Bikes_c <- bikes_m$BikePerHr_Bikes - mean(bikes_m$BikePerHr_Bikes, na.rm = T)

fit <- lm(BRRL_hr ~ BikePerHr_Bikes_c*Scenario, data = bikes_m) #fit model
resids <- fit$residuals #save residuals
fitted <- fit$fitted.values #save yhats

#repeat 5000 times
resid_resamp <- replicate(5000,{
  new_resids <- sample(resids, replace = TRUE) #resample resids w/ replacement
  bikes_m$new_y <- fitted + new_resids #add new resids to yhats to get new "data"
  fit <- lm(new_y ~ BikePerHr_Bikes_c*Scenario, data = bikes_m) #refit model
  coef(fit)
})

#estimated SEs
resid_resamp %>% t %>% as.data.frame %>% summarize_all(sd)

#empirical 95% CI
resid_resamp %>% t %>% as.data.frame %>% gather %>% group_by(key) %>% summarize(lower = quantile(value, 0.025), upper = quantile(value, 0.975))
```

# Logistic Regression on Vehicle Yielding Compliance

This logistic regression is predicting vehicle yielding compliance by considering hourly bicycle volume and scenario type. 

* 3.54902 is the odds of vehicle yielding at Test Bed 1 before bicycle signal installation.

* The odds of vehicle yielding at Test Bed 2 are 1.09576 the odds of yielding at Test Bed 1 before bicycle signal installation.

* The rest of the Test Bed coefficients follow the same line of interpretation.

* The odds of vehicle yielding for Scenario A (after) are 1.42645 the odds of yielding at Test Bed 1 before bicycle signal installation.

* The odds of vehicle yielding for Scenario A2 (second after) are 1.56985 the odds of yielding at Test Bed 1 before bicycle signal installation.

* The interaction coefficient 1.44214 is how much greater the testbed2/testbed1 ratio is after bicycle signal installation.

* The rest of the interaction coefficients with Scenario A follow the same line of interpretation.

* The interaction coefficient 0.98281 is how much greater the testbed2/testbed1 ratio in the second after bicycle signal installation scenario.

* The NAs in in the interaction terms are for scenarios that do not exist in the dataset.

```{r}
#creating the denominator for yielding compliance
bikes_m <- bikes_m %>% mutate(TY = T_VY_T + T_VFY_T) %>% mutate(y = T_VY_T/TY)

model1 <- glm(T_VY_T/TY ~ TestBed*Scenario, data = bikes_m, family = "binomial", weights = TY)

summary(model1)
coeftest(model1) %>% exp()

```
S
ince making a confusion matrix with binary regression is outside the scope of this project, I created another regression predicting whether vehicle yielding compliance was 80%+ or less than 80%. 

```{r}
#function that calculates acc,sens,spec,ppv,auc
class_diag <- function(probs,truth){ 
  #CONFUSION MATRIX: CALCULATE ACCURACY, TPR, TNR, PPV 
  if(is.character(truth)==TRUE) truth<-as.factor(truth) 
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1 
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),factor(truth, levels=c(0,1))) 
  acc=sum(diag(tab))/sum(tab) 
  sens=tab[2,2]/colSums(tab)[2] 
  spec=tab[1,1]/colSums(tab)[1] 
  ppv=tab[2,2]/rowSums(tab)[2] 
  
#CALCULATE EXACT AUC 
  ord<-order(probs, decreasing=TRUE) 
  probs <- probs[ord]; truth <- truth[ord] 
  TPR=cumsum(truth)/max(1,sum(truth))  
  FPR=cumsum(!truth)/max(1,sum(!truth)) 
  dup <-c(probs[-1]>=probs[-length(probs)], FALSE) 
  TPR <-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1) 
  n <- length(TPR) 
  auc <- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n])) 
  data.frame(acc,sens,spec,ppv,auc) 
}

```

The the accuracy of the model is about 81%, the sensitivity is about 83%, the specificity is 75%, and the precision is 88%. The AUC is in the .8-.9 range which is good.
```{r}
bikes_m <- bikes_m %>% mutate(y = ifelse(YieldVeh > .8, 1, 0))

model2 <- glm(y ~ TestBed*Scenario, data = bikes_m, family = "binomial")

summary(model2)
coeftest(model2) %>% exp()

#confusion matrix
bikes_m$probs <- predict(model2, type = "response")
pred <- ifelse(bikes_m$probs > .5, 1, 0)
table(truth = bikes_m$y, prediction = pred) %>% addmargins

class_diag(bikes_m$probs, bikes_m$y)

#ROC plot
ROCplot <- ggplot(bikes_m) + geom_roc(aes(d = y, m = probs), n.cuts = 0);ROCplot
```

# Logistic Regression Model on Yielding Compliance with More Predictors

All of the metric are great, which clearly indicates that this is an overfitted model. That's not surprising though because for 22 observations, we're considering 126 explanatory variables. 

I had to comment all of this out becaues it's not knitting. Because of an error in this chunk.
```{r, eval=F}
bikes_l <- bikes_m %>% mutate(y = ifelse(YieldVeh > .8, 1, 0)) %>% select(-STREET, -Duration_Hrs, -Start, -End, -YieldVeh, -NRTOR) 

model3 <- glm(y ~ ., data = bikes_l, family = "binomial")
bikes_l$probs <- predict(model3, type = "resp")

class_diag(bikes_l$probs, bikes_l$y)

```

For some reason, this isn't working and I can't figure out why. I tried to remove all potentially problematic columns. The error I'm getting is Error in `[.default`(tab, 2, 2) : subscript out of bounds. 
```{r, eval = F}
k=10 #choose number of folds

data<-bikes_l[sample(nrow(bikes_l)),] #randomly order rows
folds<-cut(seq(1:nrow(bikes_l)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  
  truth<-test$y ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)
  fit<-glm(y ~ .,data=train,family="binomial")
  
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth))}


summarize_all(diags,mean) #average diagnostics across all k folds
```
Performing LASSO on the same model and variables. LASSO picks `PR1_PC` & `T_VFY_Bikr`, which are the 1 party reactions per potential conflict and total number of vehicle failure to yields toward cyclists.     

I had to comment all of this out becaues it's not knitting. Because of an error in this chunk.
```{r, eval = F}

#bikes_l <- bikes_m %>% mutate(y = YieldVeh > .8, 1, 0) %>% select(-STREET, -Duration_Hrs, -NRTOR, -Start, -End, -YieldVeh) %>% na.omit
#y <- bikes_l$y %>% as.matrix #grab response
#x <- model.matrix(y ~-1+., data = bikes_l) #grab predictors, dropping intercept term
#cv <- cv.glmnet(x, y, family = "binomial") #picks an optimal value for lambda through 10-fold CV
#lasso <- glmnet(x, y, family = "binomial", lambda = cv$lambda.1se) #doing the actual lasso
#coef(lasso) #coefficients that lasso picked

#lassodat <- bikes_l %>% select(PR1_PC, T_VFY_Bikr, y)

#then just run a regression on it to predict vehicle yield > .8% from everything
#lassofit <- glm(y ~., data = lassodat, family = "binomial")
#lassoprobs <- predict(lassofit, type = "response") #get probs
#table(preds = lassoprobs >.5, truth = lassodat$y) #truth labels, and then probs >.5
#class_diag(lassoprobs, lassodat$y)
```
 Now, performing 10 fold CV only using the lasso variables. For some reason, this isn't working and I can't figure out why. I tried to remove all potentially problematic columns. The error I'm getting is Error in `[.default`(tab, 2, 2) : subscript out of bounds.
```{r}
#k=10 #choose number of folds

#data<-lassodat[sample(nrow(lassodat)),] #randomly order rows
#folds<-cut(seq(1:nrow(lassodat)),breaks=k,labels=F) #create folds

#diags<-NULL
#for(i in 1:k){
  ## Create training and test sets
  #train<-data[folds!=i,] 
  #test<-data[folds==i,]
  
  #truth<-test$y ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)
  #fit<-glm(y ~ .,data=train,family="binomial")
  
  ## Test model on test set (fold i) 
  #probs<-predict(fit,newdata = test,type="response")
  
  ## Get diagnostics for fold i
  #diags<-rbind(diags,class_diag(probs,truth))}


#summarize_all(diags,mean) #average diagnostics across all k folds
```
```
 
